@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-11},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Dosovitskiy et al_2021_An Image is Worth 16x16 Words.pdf;/Users/cameron/Zotero/storage/3H9G2WLM/2010.html}
}

@phdthesis{forbesPhysicalSocialCommonsense2021,
  type = {Thesis},
  title = {From {{Physical}} to {{Social Commonsense}}: {{Natural Language}} and the {{Natural World}}},
  shorttitle = {From {{Physical}} to {{Social Commonsense}}},
  author = {Forbes, Maxwell},
  year = {2021},
  urldate = {2022-02-09},
  abstract = {Along with the meteoric rise of computation-hungry models, NLP research has also produced new handcrafted datasets. These datasets allow us to study problems that are difficult by web scraping alone. We can use such data to evaluate and extend machine learning models into new areas. One area of natural interest is work that connects NLP to the outside world. This dissertation describes four projects that present such datasets and computational models. Each project attempts to situate NLP in a context broader than text alone. As a common thread throughout, we make use of commonsense knowledge, either explicitly or implicitly. The first half of the dissertation covers two projects, Verb Physics and Social Chemistry, which contain explicit representations of commonsense knowledge. Respectively, they capture physical commonsense (e.g., that my house is bigger than I am) and social commonsense (e.g., that it's rude for my roommate to run the blender at 5am). The second half studies language production and evaluation. In this half, commonsense implicitly informs the work. Neural Naturalist addresses language generation from image comparisons. Scarecrow focuses on evaluating text generated by large language models. In the conclusion, we urge the field to embrace communication\textemdash not merely natural language\textemdash and thereby extend the richness of groundings we consider.},
  copyright = {none},
  langid = {american},
  keywords = {p2},
  annotation = {Accepted: 2022-01-26T23:23:22Z},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Forbes (2021) From Physical to Social Commonsense.pdf;/Users/cameron/Zotero/storage/VJ9BRTP5/48229.html}
}

@article{garciaTouchWordsDynamic2016a,
  title = {A Touch with Words: {{Dynamic}} Synergies between Manual Actions and Language},
  shorttitle = {A Touch with Words},
  author = {Garc{\'i}a, Adolfo M. and Ib{\'a}{\~n}ez, Agust{\'i}n},
  year = {2016},
  month = sep,
  journal = {Neuroscience and Biobehavioral Reviews},
  volume = {68},
  pages = {59--95},
  issn = {1873-7528},
  doi = {10/f829ht},
  abstract = {Manual actions are a hallmark of humanness. Their underlying neural circuitry gives rise to species-specific skills and interacts with language processes. In particular, multiple studies show that hand-related expressions - verbal units evoking manual activity - variously affect concurrent manual actions, yielding apparently controversial results (interference, facilitation, or null effects) in varied time windows. Through a systematic review of 108 experiments, we show that such effects are driven by several factors, such as the level of verbal processing, action complexity, and the time-lag between linguistic and motor processes. We reconcile key empirical patterns by introducing the Hand-Action-Network Dynamic Language Embodiment (HANDLE) model, an integrative framework based on neural coupling dynamics and predictive-coding principles. To conclude, we assess HANDLE against the backdrop of other action-cognition theories, illustrate its potential applications to understand high-level deficits in motor disorders, and discuss key challenges for further development. In sum, our work aligns with the 'pragmatic turn', moving away from passive and static representationalist perspectives to a more dynamic, enactive, and embodied conceptualization of cognitive processes.},
  langid = {english},
  pmid = {27189784},
  keywords = {Cognition,Comprehension,Concept Formation,Enactive cognition,Hand-related language,Language,Language embodiment,Manual-action networks,Predictive coding},
  annotation = {ZSCC: 0000082}
}

@inproceedings{girdharImagebindOneEmbedding2023,
  title = {Imagebind: {{One}} Embedding Space to Bind Them All},
  shorttitle = {Imagebind},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girdhar, Rohit and {El-Nouby}, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  year = {2023},
  pages = {15180--15190},
  urldate = {2023-09-28},
  keywords = {⛔ No DOI found},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Girdhar et al_2023_Imagebind.pdf}
}

@misc{girdharImageBindOneEmbedding2023,
  title = {{{ImageBind}}: {{One Embedding Space To Bind Them All}}},
  shorttitle = {{{ImageBind}}},
  author = {Girdhar, Rohit and {El-Nouby}, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  year = {2023},
  month = may,
  number = {arXiv:2305.05665},
  eprint = {2305.05665},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-10},
  abstract = {We present IMAGEBIND, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. IMAGEBIND can leverage recent large scale vision-language models, and extends their zeroshot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications `out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-theart on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that IMAGEBIND serves as a new way to evaluate vision models for visual and non-visual tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {/Users/cameron/Zotero/storage/ZRDFXKL5/Girdhar et al. - 2023 - ImageBind One Embedding Space To Bind Them All.pdf}
}

@article{guoFastExplicitNeural2021,
  title = {Fast and {{Explicit Neural View Synthesis}}},
  author = {Guo, Pengsheng and Bautista, Miguel Angel and Colburn, Alex and Yang, Liang and Ulbricht, Daniel and Susskind, Joshua M. and Shan, Qi},
  year = {2021},
  journal = {arXiv preprint arXiv:2107.05775},
  eprint = {2107.05775},
  archiveprefix = {arxiv},
  annotation = {ZSCC: 0000000},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Guo et al (2021) Fast and Explicit Neural View Synthesis.pdf;/Users/cameron/Zotero/storage/XSKI3FS8/2107.html}
}

@misc{IMAGEBINDOneEmbedding,
  title = {{{IMAGEBIND}}: {{One Embedding Space To Bind Them All}} - {{Meta Research}}},
  shorttitle = {{{IMAGEBIND}}},
  journal = {Meta Research},
  urldate = {2023-09-28},
  abstract = {We present IMAGEBIND, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together.},
  howpublished = {https://research.facebook.com/publications/imagebind-one-embedding-space-to-bind-them-all/},
  langid = {english},
  file = {/Users/cameron/Zotero/storage/NUHKP3IM/imagebind-one-embedding-space-to-bind-them-all.html}
}

@article{lucyAreDistributionalRepresentations2017,
  title = {Are Distributional Representations Ready for the Real World? {{Evaluating}} Word Vectors for Grounded Perceptual Meaning},
  shorttitle = {Are Distributional Representations Ready for the Real World?},
  author = {Lucy, Li and Gauthier, Jon},
  year = {2017},
  month = may,
  journal = {arXiv:1705.11168 [cs]},
  eprint = {1705.11168},
  primaryclass = {cs},
  urldate = {2022-01-30},
  abstract = {Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,archived,Computer Science - Computation and Language},
  annotation = {ZSCC: 0000045},
  file = {/Users/cameron/Documents/Papers/Bibliographies/cogsci_2022_affordances_nlm/Lucy Gauthier (2017) Are distributional representations ready for the real world.pdf;/Users/cameron/Zotero/storage/N64XIQ4E/1705.html}
}

@misc{radfordLearningTransferableVisual2021a,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-10},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/cameron/Zotero/storage/AYCHXAPP/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf}
}

@article{recchiaTeachingAutoregressiveLanguage2021,
  title = {Teaching {{Autoregressive Language Models Complex Tasks By Demonstration}}},
  author = {Recchia, Gabriel},
  year = {2021},
  month = sep,
  urldate = {2022-01-15},
  abstract = {This paper demonstrates that by fine-tuning an autoregressive language model (GPT-Neo) on appropriately structured step-by-step demonstrations, it is possible to teach it to execute a mathematical task that has previously proved difficult for Transformers - longhand modulo operations - with a relatively small number of examples. Specifically, we fine-tune GPT-Neo to solve the numbers\_\_div\_remainder task from the DeepMind Mathematics Dataset; Saxton et al. (arXiv:1904.01557) reported below 40\% accuracy on this task with 2 million training examples. We show that after fine-tuning on 200 appropriately structured demonstrations of solving long division problems and reporting the remainders, the smallest available GPT-Neo model achieves over 80\% accuracy. This is achieved by constructing an appropriate dataset for fine-tuning, with no changes to the learning algorithm. These results suggest that fine-tuning autoregressive language models on small sets of well-crafted demonstrations may be a useful paradigm for enabling individuals without training in machine learning to coax such models to perform some kinds of complex multi-step tasks.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000000},
  file = {/Users/cameron/Zotero/storage/QSBXNYEU/Recchia - 2021 - Teaching Autoregressive Language Models Complex Ta.pdf;/Users/cameron/Zotero/storage/W4K63KMS/2109.html}
}

@article{schuhmannLaion5bOpenLargescale2022,
  title = {Laion-5b: {{An}} Open Large-Scale Dataset for Training next Generation Image-Text Models},
  shorttitle = {Laion-5b},
  author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {25278--25294},
  urldate = {2023-10-11},
  keywords = {⛔ No DOI found},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Schuhmann et al_2022_Laion-5b.pdf}
}

@article{suttonBitterLesson2019,
  title = {The Bitter Lesson},
  author = {Sutton, Richard},
  year = {2019},
  journal = {Incomplete Ideas (blog)},
  volume = {13},
  number = {1},
  urldate = {2023-09-28},
  keywords = {⛔ No DOI found},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Sutton_2019_The bitter lesson.pdf}
}

@article{wangLanguageMediatedObjectCentricRepresentation2020,
  title = {Language-{{Mediated}}, {{Object-Centric Representation Learning}}},
  author = {Wang, Ruocheng and Mao, Jiayuan and Gershman, Samuel J. and Wu, Jiajun},
  year = {2020},
  journal = {arXiv preprint arXiv:2012.15814},
  eprint = {2012.15814},
  archiveprefix = {arxiv},
  annotation = {ZSCC: 0000003},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Wang et al (2020) Language-Mediated, Object-Centric Representation Learning.pdf;/Users/cameron/Zotero/storage/AAMANQRN/2012.html}
}
